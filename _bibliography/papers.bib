---
---

@article{hwang2023identifying,
  title={Identifying household finance heterogeneity via deep clustering},
  author={Hwang, Yoontae and Lee, Yongjae and Fabozzi, Frank J},
  journal={Annals of Operations Research},
  abstract={Households are becoming increasingly heterogeneous. While previous studies have revealed many important insights (e.g., wealth effect, income effect), they could only incorporate two or three variables at a time. However, in order to have a more detailed understanding of complex household heterogeneity, more variables should be considered simultaneously. In this study, we argue that advanced clustering techniques can be useful for investigating high-dimensional household heterogeneity. A deep learning-based clustering method is used to effectively handle the high-dimensional balance sheet data of approximately 50,000 households. The employment of appropriate dimension-reduction techniques is the key to incorporate the full joint distribution of high-dimensional data in the clustering step. Our study suggests that various variables should be used together to explain household heterogeneity. Asset variables are found to be crucial for understanding heterogeneity within wealthy households, while debt variables are more important for those households that are not wealthy. In addition, relationships with sociodemographic variables (e.g., age, education, and family size) were further analyzed. Although clusters are found only based on financial variables, they are shown to be closely related to most sociodemographic variables.}
  volume={325},
  number={2},
  pages={1255--1289},
  year={2023},
  publisher={Springer}
}


@inproceedings{StengelEskin:2022:troubling_quirk,
  title         = {{When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems}},
  author        = {Elias Stengel-Eskin and Emmanouil Antonios Platanios and Adam Pauls and Sam Thomson and Hao Fang and Benjamin Van Durme and Jason Eisner and Yu Su},
  booktitle     = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  abbr          = {EMNLP},
  year          = {2022},
  abstract      = {In natural language understanding (NLU) production systems, users' evolving needs necessitate the addition of new features over time, indexed by new symbols added to the meaning representation space. This requires additional training data and results in ever-growing datasets. We present the first systematic investigation into this incremental symbol learning scenario. Our analyses reveal a troubling quirk in building (broad-coverage) NLU systems: as the training dataset grows, more data is needed to learn new symbols, forming a vicious cycle. We show that this trend holds for multiple mainstream models on two common NLU tasks: intent recognition and semantic parsing. Rejecting class imbalance as the sole culprit, we reveal that the trend is closely associated with an effect we call source signal dilution, where strong lexical cues for the new symbol become diluted as the training dataset grows. Selectively dropping training examples to prevent dilution often reverses the trend, showing the over-reliance of mainstream neural NLU models on simple lexical cues and their lack of contextual understanding.},
  pdf           = {stengel_eskin_2022_troubling_quirk/paper.pdf},
  code          = {https://github.com/microsoft/nlu-incremental-symbol-learning},
}
